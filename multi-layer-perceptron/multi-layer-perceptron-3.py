# -*- coding: utf-8 -*-
"""Q3-hi3334.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19lBBNprrzUIoT9zvDBywpzN8kFG6qF6r
"""

import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt

# load dataset 
X_training_Data = pd.read_csv("Training_DB_Multifont inputs.csv", header = None)
Y_training_Data = pd.read_csv("Training_DB_Multifont outputs.csv", header = None)
X_testing_Data = pd.read_csv("Testing_DB_Multifont inputs.csv", header = None)
Y_testing_Data = pd.read_csv("Testing_DB_Multifont oututs.csv", header = None)

#nb_classes = 26  # Number of Alphabet: 0 ~ 25
X = tf.placeholder(tf.float32, [None, 14])  # Number of colum: 14
Y = tf.placeholder(tf.float32, [None, 26])  # Number of Alphabet: 26 - 0 ~ 25

#self._states = tf.Variable(tf.ones(shape=[None, self._num_states]), dtype=tf.float32)

# 3 hidden layers using xavier and relu
W1 = tf.get_variable("W1", shape=[14, 52], initializer=tf.contrib.layers.xavier_initializer())
b1 = tf.Variable(tf.random_normal([52]), name='bias')
L1 = tf.nn.relu(tf.matmul(X, W1) + b1)

W2 = tf.get_variable("W2", shape =[52, 13], initializer=tf.contrib.layers.xavier_initializer())
b2 = tf.Variable(tf.random_normal([13]), name='bias')
L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)

W3 = tf.get_variable("W3", shape =[13, 26], initializer=tf.contrib.layers.xavier_initializer())
b3 = tf.Variable(tf.random_normal([26]), name='bias')

# tf.nn.softmax computes softmax activations
# softmax = exp(logits) / reduce_sum(exp(logits), dim)
logits = tf.matmul(L2, W3) + b3
hypothesis = tf.nn.softmax(logits)

# Cross entropy loss
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))
optimizer = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(cost)

# Prediction
prediction = tf.argmax(hypothesis, 1)
correct_prediction = tf.equal(prediction, tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    
# Launch graph
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for step in range(500):
        sess.run(optimizer, feed_dict={X: X_training_Data, Y: Y_training_Data})
        if step % 5 == 0:
            loss, acc = sess.run([cost, accuracy], feed_dict={X: X_training_Data, Y: Y_training_Data})
            print("Step: {:5}\tLoss: {:.3f}\t Train_Acc: {:.2%}".format(step, loss, acc))
            
# Test set prediction
    loss, acc = sess.run([cost, accuracy], feed_dict={X: X_testing_Data, Y: Y_testing_Data})
    pred = sess.run(prediction, feed_dict={X: X_testing_Data, Y: Y_training_Data})
    print("Step: {:5}\tLoss: {:.3f}\t Test_Acc: {:.2%}".format(step, loss, acc))